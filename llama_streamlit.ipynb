{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0215f9",
   "metadata": {},
   "source": [
    "<font size=\"5.2\">Llama-Powered Customer Chatbot</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9a567",
   "metadata": {},
   "source": [
    "This Streamlit app functions as a question-answering chatbot tailored for customer support. It is powered by the TinyLlama-1.1B-Chat-v1.0 model, fine-tuned using the Bitext Customer Support LLM Chatbot dataset. \n",
    "\n",
    "You can explore the deployed app on the Streamlit community here: [link].\n",
    "\n",
    "The chatbot categorizes customer questions into the following question:\n",
    "\n",
    "* **HIPPING**\n",
    "* **CANCELLATION**\n",
    "* **INVOICE**\n",
    "* **PAYMENT**\n",
    "* **REFUND**\n",
    "* **FEEDBACK**\n",
    "* **CONTACT**\n",
    "* **ACCOUNT**\n",
    "* **DELIVERY**\n",
    "* **SUBSCRIPTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd8825",
   "metadata": {},
   "source": [
    "Watch the video below to see how to use the web app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ace337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"900\" controls>\n",
       "  <source src=\"streamlit-llama_video.webm\" type=\"video/webm\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"900\" controls>\n",
    "  <source src=\"{\"streamlit-llama_video.webm\"}\" type=\"video/webm\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c77c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Streamlit app for question-answering using llama\"\"\"\n",
    "\n",
    "# Import packages required across different parts of app\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import streamlit\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# set up page configuration\n",
    "st.set_page_config(page_title=\"Llama-Powered Customer Chatbot\",\n",
    "    page_icon=\"ðŸ¤–\",\n",
    "    layout=\"wide\",\n",
    "    #layout=\"centered\"\n",
    ")\n",
    "\n",
    "st.title('Llama-Powered Customer Chatbot ðŸ¤–')\n",
    "\n",
    "# Typing effect that stops at the author's name length and repeats from the beginning\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "        .author-title {\n",
    "            font-size: 1.3em;\n",
    "            font-weight: bold;\n",
    "            color: #007acc; /* Color for \"Author:\" */\n",
    "            white-space: nowrap;\n",
    "            vertical-align: middle; /* Ensures alignment with animated text */\n",
    "        }\n",
    "\n",
    "        .author-name {\n",
    "            font-size: 1.2em;\n",
    "            font-weight: bold;\n",
    "            color: red; /* Color for the author's name */\n",
    "            overflow: hidden;\n",
    "            white-space: nowrap;\n",
    "            border-right: 3px solid;\n",
    "            display: inline-block;\n",
    "            vertical-align: middle; /* Aligns with the static \"Author:\" text */\n",
    "            animation: typing 5s steps(20, end) infinite, blink-caret 0.75s step-end infinite;\n",
    "            max-width: 10ch; /* Limit width to fit text length */\n",
    "        }\n",
    "\n",
    "        /* Typing effect */\n",
    "        @keyframes typing {\n",
    "            0% { max-width: 0; }\n",
    "            50% { max-width: 30ch; } /* Adjust to match the name's length */\n",
    "            100% { max-width: 0; } /* Reset back to zero */\n",
    "        }\n",
    "\n",
    "        /* Blinking cursor animation for the author's name */\n",
    "        @keyframes blink-caret {\n",
    "            from, to { border-color: transparent; }\n",
    "            50% { border-color: red; }\n",
    "        }\n",
    "    </style>\n",
    "\n",
    "    <p><span class=\"author-title\">Author:</span> \\\n",
    "    <span class=\"author-name\">Mehdi Rezvandehy</span></p>\n",
    "\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "# Table of contents\n",
    "st.sidebar.title(\"Table of Contents\")\n",
    "st.sidebar.markdown(\"[**Dataset**](#dataset)\")\n",
    "st.sidebar.markdown(\"[**Select Llama Model Configuration**](#select-llama-model-configuration)\")\n",
    "st.sidebar.markdown(\"[**Question-Answering Chatbot**](#question-answering-chatbot)\")\n",
    "\n",
    "\n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "st.markdown(\"\"\"\n",
    "This app, built with [Streamlit](https://streamlit.io/), serves as a question-answering chatbot \n",
    "designed for customer support. The **TinyLlama-1.1B-Chat-v1.0** model, which has been fine-tuned using labeled data.\n",
    "\"\"\"\n",
    ")\n",
    "st.header(' Dataset')\n",
    "st.markdown(\"\"\"\n",
    "The [Bitext Customer Support LLM Chatbot Training](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset) \n",
    "dataset is a comprehensive resource for developing conversational AI models tailored to customer service needs. It features fields such as `instruction` \n",
    "(representing user queries), `response` (providing model replies), `category` (for semantic grouping), and `intent` \n",
    "(to capture specific user intents). The dataset supports various customer service scenarios, including account management, \n",
    "refunds, invoices, and order processing. It consists of structured question-answer pairs created through a hybrid \n",
    "approach, combining natural language processing with generation techniques, and curated by computational linguists. This dataset \n",
    "was used to fine-tune **TinyLlama-1.1B-Chat-v1.0**.\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "st.image('bitext_customer.jpg')\n",
    "\n",
    "\n",
    "# Custom CSS for centering and resizing the button\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .stButton > button {\n",
    "        padding: 1rem 5rem;\n",
    "        font-size: 16px !important; /* Forces font size */\n",
    "        font-weight: bold; /* Makes font bolder */\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "if 'load_fine_tuned_model' not in st.session_state:\n",
    "    with st.spinner(\"Loading fine-tuned model...\"):\n",
    "        st.session_state.load_fine_tuned_model = True\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        # Specify the path where you saved the model and tokenizer\n",
    "        model_path = \"saved_model\"\n",
    "        \n",
    "        # Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        st.session_state.tokenizer = tokenizer\n",
    "        \n",
    "        # Load the model\n",
    "        model_fine_tuned = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        st.session_state.model_fine_tuned = model_fine_tuned\n",
    "\n",
    "st.header(\"Select Llama Model Configuration\")\n",
    "if st.session_state.load_fine_tuned_model:\n",
    "    from transformers import pipeline\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "    # run a simple LLMChain\n",
    "    from langchain.chains import LLMChain\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Use st.number_input for user input with a default value as float\n",
    "    col1, col2 = st.columns([1, 1])  # Adjust the list to control column widths\n",
    "    \n",
    "    with col1:\n",
    "        # Display a constant, non-editable text input field\n",
    "        temperature = st.number_input(\"temperature\", value=0.0)\n",
    "        max_length = st.number_input(\"max_length\", value=200)\n",
    "    \n",
    "    with col2:\n",
    "        top_p = st.number_input(\"top_p\", value=0.8, min_value=0.0, max_value=1.0)\n",
    "        top_k = st.number_input(\"top_k\", value=1, min_value=1, max_value=50)\n",
    "    \n",
    "    # Create a text-generation pipeline\n",
    "    pipe = pipeline(\"text-generation\", \n",
    "                    model=st.session_state.model_fine_tuned , \n",
    "                    tokenizer=st.session_state.tokenizer,\n",
    "                    # Adjust generation parameters here\n",
    "                    temperature=temperature,  # Adjust the temperature\n",
    "                    top_k=top_k,              # Set top_k for controlling sampling diversity\n",
    "                    top_p=top_p,              # Use nucleus sampling with top_p\n",
    "                    max_length=max_length,    # Max number of tokens to generate\n",
    "                   )\n",
    "    \n",
    "    \n",
    "    # Question-Answering Chatbot\n",
    "    st.header('Question-Answering Chatbot')\n",
    "    \n",
    "    \n",
    "    # Apply custom CSS for increasing text input box width\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "        div[data-baseweb=\"input\"] {\n",
    "            width: 600px !important; /* Adjust the width as needed */\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "    \n",
    "    user_input = st.text_area(\"Ask me questions for customer support ðŸ‘‡\")\n",
    "    response = st.selectbox(\"\"\"Does your question pertain to any of the following topics: \n",
    "        ORDER, SHIPPING, CANCELLATION, INVOICE, PAYMENT, REFUND, FEEDBACK, \n",
    "        CONTACT, ACCOUNT, DELIVERY, or SUBSCRIPTION?\"\"\",options=[\"No\", \"Yes\"], index=0)\n",
    "    if response == \"Yes\":\n",
    "        pass\n",
    "    else:\n",
    "        st.error(\"Action canceled. Please submit only questions \\\n",
    "            related to the topics listed above.\")\n",
    "    \n",
    "    if user_input and response ==\"Yes\" and st.button('Generate Response'):\n",
    "        from langchain.chains import ConversationChain\n",
    "        from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "        # Wrap it for LangChain\n",
    "        llm_chat = HuggingFacePipeline(pipeline=pipe)\n",
    "        \n",
    "        # Define a prompt template\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"Question: {question}\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        # Create the chain\n",
    "        chain = prompt | llm_chat\n",
    "        \n",
    "        # initialize memory\n",
    "        #memory = ConversationBufferWindowMemory(k=3)\n",
    "        #conversation = ConversationChain(\n",
    "        #    llm=chain,\n",
    "        #    memory=memory,\n",
    "        #    verbose=True # see what is going on in background\n",
    "        #)\n",
    "\n",
    "\n",
    "        with st.spinner(\"Please wait, processing...\"):\n",
    "            if 'produced_doc' not in st.session_state:\n",
    "                st.session_state['produced_doc'] = []\n",
    "    \n",
    "            if 'old_doc' not in st.session_state:\n",
    "                st.session_state['old_doc'] = []\n",
    "    \n",
    "            #llm_response = conversation.invoke({\"input\": user_input})\n",
    "            response = chain.invoke({\"question\": user_input})\n",
    "            answer = response.split('Answer: ')[1]\n",
    "            #latest_response = llm_response.get(\"response\")\n",
    "            #Answer = latest_response.split('Answer: ')[1]\n",
    "            st.session_state.old_doc.append(user_input)\n",
    "            st.session_state.produced_doc.append(answer)\n",
    "    \n",
    "            # Button to clear history\n",
    "            if st.button(\"Clear History\", key=\"Run\"):\n",
    "                #conversation.memory.clear()\n",
    "                st.session_state['old_doc'] = []  # Clear user questions\n",
    "                st.session_state['produced_doc'] = []  # Clear model responses\n",
    "    \n",
    "            if st.session_state['produced_doc']:\n",
    "                for i in range(len(st.session_state['produced_doc'])):\n",
    "                    message(st.session_state['old_doc'][i],\n",
    "                            is_user=True, key=str(i)+ '_old_user')\n",
    "                    message(st.session_state['produced_doc'][i],\n",
    "                            key=str(i)+ '_prod_user')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd7af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.596px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
